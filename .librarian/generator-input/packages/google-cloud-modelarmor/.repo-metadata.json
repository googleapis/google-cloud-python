{
    "name": "google-cloud-modelarmor",
    "name_pretty": "Model Armor API",
    "api_description": "Model Armor helps you protect against risks like prompt injection, harmful content, and data leakage in generative AI applications by letting you define policies that filter user prompts and model responses.",
    "product_documentation": "https://cloud.google.com/security-command-center/docs/model-armor-overview",
    "client_documentation": "https://cloud.google.com/python/docs/reference/google-cloud-modelarmor/latest",
    "issue_tracker": "https://issuetracker.google.com/issues/new?component=1514910&template=0",
    "release_level": "preview",
    "language": "python",
    "library_type": "GAPIC_AUTO",
    "repo": "googleapis/google-cloud-python",
    "distribution_name": "google-cloud-modelarmor",
    "api_id": "securitycenter.googleapis.com",
    "default_version": "v1",
    "codeowner_team": "",
    "api_shortname": "securitycenter"
}
